{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFLumuXmVduUyAN7MtWMlK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yjOIlkbZ0Tr","executionInfo":{"status":"ok","timestamp":1762527576307,"user_tz":-300,"elapsed":28744,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}},"outputId":"34d10932-9717-48c5-8114-a384bd08b978"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","\n","folder_path = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/Drowninss-dataset/train\"\n","\n","image_count = 0\n","\n","for root, dirs, files in os.walk(folder_path):\n","    for file in files:\n","        if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n","            image_count += 1\n","\n","print(\"Total images in all classes:\", image_count)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmj4aakAM7qD","executionInfo":{"status":"ok","timestamp":1762413319251,"user_tz":-300,"elapsed":6434,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}},"outputId":"361877b8-bfce-4455-ec31-d532cef4acea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total images in all classes: 2900\n"]}]},{"cell_type":"code","source":["pip install opencv-python mediapipe tensorflow numpy matplotlib scikit-learn playsound\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rePruc2Jcz6d","executionInfo":{"status":"ok","timestamp":1762527590515,"user_tz":-300,"elapsed":13954,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}},"outputId":"6df5d904-f929-47c6-b0f3-51492cc76425"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Collecting playsound\n","  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n","INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n","Building wheels for collected packages: playsound\n","  Building wheel for playsound (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7020 sha256=105391bc7de56899283e0d6b2d06ccde2290449a6f093651b5a6be445f60009c\n","  Stored in directory: /root/.cache/pip/wheels/cf/42/ff/7c587bae55eec67b909ca316b250d9b4daedbf272a3cbeb907\n","Successfully built playsound\n","Installing collected packages: playsound, protobuf, sounddevice, mediapipe\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mediapipe-0.10.14 playsound-1.3.0 protobuf-4.25.8 sounddevice-0.5.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"8363334668534f08a01cb22f3d75b8c7"}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mD5BCdJXGd1U","executionInfo":{"status":"ok","timestamp":1762340722113,"user_tz":-300,"elapsed":2319000,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}},"outputId":"834521d4-99cd-4672-d12e-d76cbe61f8c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1162 images belonging to 2 classes.\n","Found 290 images belonging to 2 classes.\n","Eye class_indices: {'Closed': 0, 'Open': 1}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.5163 - loss: 1.0392 - val_accuracy: 0.6138 - val_loss: 0.6491\n","Epoch 2/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.6025 - loss: 0.7202 - val_accuracy: 0.7690 - val_loss: 0.5337\n","Epoch 3/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.7020 - loss: 0.5783 - val_accuracy: 0.8379 - val_loss: 0.4663\n","Epoch 4/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.7543 - loss: 0.5010 - val_accuracy: 0.8862 - val_loss: 0.3891\n","Epoch 5/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.7949 - loss: 0.4494 - val_accuracy: 0.9172 - val_loss: 0.3498\n","Epoch 6/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.8194 - loss: 0.4175 - val_accuracy: 0.9276 - val_loss: 0.3119\n","Epoch 7/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.8760 - loss: 0.3466 - val_accuracy: 0.9379 - val_loss: 0.2746\n","Epoch 8/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.8800 - loss: 0.3206 - val_accuracy: 0.9345 - val_loss: 0.2671\n","Epoch 9/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.8980 - loss: 0.2841 - val_accuracy: 0.9310 - val_loss: 0.2429\n","Epoch 10/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.9359 - loss: 0.2357 - val_accuracy: 0.9517 - val_loss: 0.2221\n","Epoch 11/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.9194 - loss: 0.2490 - val_accuracy: 0.9483 - val_loss: 0.2137\n","Epoch 12/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.9529 - loss: 0.1974 - val_accuracy: 0.9379 - val_loss: 0.2041\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/eye_open_closed.h5\n","\n","=== Eye (Closed vs Open) – Classification Report ===\n","              precision    recall  f1-score   support\n","\n","           0      0.900     0.993     0.944       145\n","           1      0.992     0.890     0.938       145\n","\n","    accuracy                          0.941       290\n","   macro avg      0.946     0.941     0.941       290\n","weighted avg      0.946     0.941     0.941       290\n","\n","Confusion matrix:\n"," [[144   1]\n"," [ 16 129]]\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmpdxeaum3p'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_2')\n","Output Type:\n","  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n","Captures:\n","  137706520461584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706430545744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070996304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070994576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706520461776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070993040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067139728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067141072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067144912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067145488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067136464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067130704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067139536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067141456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067140688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067140496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067138768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067133776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067137424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067142800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067138960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067138000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067139152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067138576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067139920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067131856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070244048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070242896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067146064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070243088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070244816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070246544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707103512016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070246736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070251152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707090978640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070243472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070247888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070245968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070246928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070245392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431283472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431283664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070243664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070245008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431283280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431299216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431299024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418128272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418128464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431299408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418127312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418126928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418128080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418127888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418127696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418129424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418128848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418129040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418129232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418127120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418130384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418129808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418130000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418130192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418127504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418131344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418130768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418130960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418131152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418128656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418132304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418131728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418131920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418132112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418129616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418133264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418132688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418132880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418133072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418130576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418134224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418133648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418133840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418134032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418131536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418135184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418134608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418134800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418134992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418132496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418136144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418135568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418135760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418135952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418133456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418137104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418136528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418136720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418136912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418134416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418138064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418137488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418137680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418137872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418135376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418139024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418138448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418138640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418138832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418136336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418139984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418139408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418139600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418139792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418137296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418140944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418140368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418140560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418140752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418138256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418141904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418141328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418141520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418141712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418139216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418142864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418142288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418141136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418142672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418142480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418142096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418140176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418143056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418454800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418454992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418454608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418461904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418461328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418461520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418461712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418462864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418462288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418462480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418462672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418463824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418463248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418463440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418463632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418461136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418464784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418464208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418464400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418464592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418462096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418465744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418465168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418463056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","Saved: /content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/eye_open_closed.tflite\n","Found 1159 images belonging to 2 classes.\n","Found 289 images belonging to 2 classes.\n","Yawn class_indices: {'no_yawn': 0, 'yawn': 1}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 3s/step - accuracy: 0.5054 - loss: 0.8329 - val_accuracy: 0.5433 - val_loss: 0.7135\n","Epoch 2/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.5103 - loss: 0.7936 - val_accuracy: 0.5087 - val_loss: 0.7139\n","Epoch 3/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.5446 - loss: 0.7680 - val_accuracy: 0.5606 - val_loss: 0.6728\n","Epoch 4/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.5538 - loss: 0.7350 - val_accuracy: 0.5190 - val_loss: 0.6981\n","Epoch 5/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.5423 - loss: 0.7613 - val_accuracy: 0.5571 - val_loss: 0.6647\n","Epoch 6/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.5490 - loss: 0.7198 - val_accuracy: 0.5779 - val_loss: 0.6550\n","Epoch 7/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.6012 - loss: 0.6871 - val_accuracy: 0.6055 - val_loss: 0.6543\n","Epoch 8/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.6000 - loss: 0.7050 - val_accuracy: 0.6401 - val_loss: 0.6307\n","Epoch 9/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.5705 - loss: 0.7243 - val_accuracy: 0.5640 - val_loss: 0.6484\n","Epoch 10/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.5973 - loss: 0.7024 - val_accuracy: 0.6436 - val_loss: 0.6345\n","Epoch 11/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.5998 - loss: 0.6833 - val_accuracy: 0.6055 - val_loss: 0.6277\n","Epoch 12/12\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.6266 - loss: 0.6457 - val_accuracy: 0.5986 - val_loss: 0.6426\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/yawn_no_yawn.h5\n","\n","=== Yawn (no_yawn vs yawn) – Classification Report ===\n","              precision    recall  f1-score   support\n","\n","           0      0.569     0.566     0.567       145\n","           1      0.566     0.569     0.567       144\n","\n","    accuracy                          0.567       289\n","   macro avg      0.567     0.567     0.567       289\n","weighted avg      0.567     0.567     0.567       289\n","\n","Confusion matrix:\n"," [[82 63]\n"," [62 82]]\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmpjxp6020t'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_3')\n","Output Type:\n","  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n","Captures:\n","  137706385772560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706300880464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706300879888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706300882768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706300878160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706300882192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305283920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305284496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706300879312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706300882000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305285072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305283536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305283344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305283728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305282192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305282384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305282960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305282768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305283152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305281424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305284688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305284112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305281616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305284880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305280464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305281808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305281232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305280656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305282576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305279504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305280848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305280272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305279696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305284304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305278544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305279888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305279312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305278736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305282000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305277584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305278928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305278352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305277776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305281040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305276624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305277968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305277392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305276816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305280080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305275664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305277008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305276432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305275856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305279120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305274704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305276048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305275472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305274896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305278160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305274320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706534789648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305274128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706534789264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305273936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305277200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706534791760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706430544016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305276240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706430545744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305275280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305274512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070994576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706430544208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070993040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706430545168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706305275088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070996112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067145488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706430544400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070996304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067141072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067130704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067136464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067139728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067133776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067140688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067140496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067138768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067139536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067139152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067142800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067138960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707090978640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707103512016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067138576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067137424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067139920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067131856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067146064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067138000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067141456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070243088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070244048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707067144912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070247888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070244816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070251152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070246928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070246736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070245008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070243472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070245392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070242896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070243664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070245968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137707070246544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431283472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431283664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431283280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431285776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431286928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431287888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431288848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431289808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431290768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431291728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431292688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431299024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431299216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431295568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431299408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431294608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706520461776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431298448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706520461392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431296528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431297488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706520461968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418465360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706431293648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706520461584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418464976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418466896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418467472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418464016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418467664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418467856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418468240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418468432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418467280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418469008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418468816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418469392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418469584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418466704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418469968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418469776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418468624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418470352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418465552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418470544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418470736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418468048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418454992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418454800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418469200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418454608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418470160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418455184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418456336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418457296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418458256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418461328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418461520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418461712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418459984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418462288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  137706418460944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","Saved: /content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/yawn_no_yawn.tflite\n","\n","All done. Models in: /content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project\n"]}],"source":["# train_dual_models.py\n","import os, numpy as np, matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# -------- config --------\n","DATASET_ROOT = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/Drowninss-dataset/train\"       # change if needed\n","IMG_SIZE     = 224\n","BATCH        = 32\n","EPOCHS       = 12\n","LR           = 1e-4\n","VAL_SPLIT    = 0.20\n","OUTDIR       = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project\"\n","os.makedirs(OUTDIR, exist_ok=True)\n","\n","def build_base():\n","    base = MobileNetV2(input_shape=(IMG_SIZE,IMG_SIZE,3), include_top=False, weights='imagenet')\n","    base.trainable = False\n","    x = GlobalAveragePooling2D()(base.output)\n","    x = Dropout(0.4)(x)\n","    out = Dense(1, activation='sigmoid')(x)\n","    model = Model(base.input, out)\n","    model.compile(optimizer=Adam(LR), loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def make_gens(parent_dir, classes, val_split=VAL_SPLIT):\n","    dg = ImageDataGenerator(\n","        rescale=1./255,\n","        validation_split=val_split,\n","        rotation_range=15,\n","        width_shift_range=0.1, height_shift_range=0.1,\n","        zoom_range=0.2, horizontal_flip=True,\n","    )\n","    train_gen = dg.flow_from_directory(\n","        parent_dir, classes=classes, target_size=(IMG_SIZE,IMG_SIZE),\n","        batch_size=BATCH, class_mode='binary', subset='training', shuffle=True)\n","    val_gen = dg.flow_from_directory(\n","        parent_dir, classes=classes, target_size=(IMG_SIZE,IMG_SIZE),\n","        batch_size=BATCH, class_mode='binary', subset='validation', shuffle=False)\n","    return train_gen, val_gen\n","\n","def get_class_weights(gen):\n","    y = gen.classes\n","    classes = np.unique(y)\n","    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n","    return {int(c): float(w) for c, w in zip(classes, weights)}\n","\n","def evaluate(model, val_gen, title):\n","    val_gen.reset()\n","    y_true = val_gen.classes\n","    y_prob = model.predict(val_gen, verbose=0).ravel()\n","    y_pred = (y_prob >= 0.5).astype(int)\n","    print(f\"\\n=== {title} – Classification Report ===\")\n","    print(classification_report(y_true, y_pred, digits=3))\n","    print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n","    # small accuracy plot not required; kept simple\n","    return y_true, y_pred\n","\n","def to_tflite(h5_path):\n","    try:\n","        m = tf.keras.models.load_model(h5_path)\n","        conv = tf.lite.TFLiteConverter.from_keras_model(m)\n","        conv.optimizations = [tf.lite.Optimize.DEFAULT]\n","        tfl = conv.convert()\n","        tpath = h5_path.replace(\".h5\", \".tflite\")\n","        open(tpath, \"wb\").write(tfl)\n","        print(\"Saved:\", tpath)\n","    except Exception as e:\n","        print(\"TFLite conversion failed:\", e)\n","\n","if __name__ == \"__main__\":\n","    # Eye model (IMPORTANT order): [\"Closed\",\"Open\"] → class 0=Closed, 1=Open\n","    eye_classes = [\"Closed\", \"Open\"]\n","    eye_train, eye_val = make_gens(DATASET_ROOT, eye_classes)\n","    print(\"Eye class_indices:\", eye_train.class_indices)  # {'Closed':0,'Open':1}\n","    eye_cw = get_class_weights(eye_train)\n","    eye_model = build_base()\n","    eye_hist = eye_model.fit(eye_train, validation_data=eye_val, epochs=EPOCHS,\n","                             class_weight=eye_cw, verbose=1)\n","    eye_h5 = os.path.join(OUTDIR, \"eye_open_closed.h5\")\n","    eye_model.save(eye_h5); print(\"Saved:\", eye_h5)\n","    evaluate(eye_model, eye_val, \"Eye (Closed vs Open)\")\n","    to_tflite(eye_h5)\n","\n","    # Yawn model (IMPORTANT order): [\"no_yawn\",\"yawn\"] → class 0=no_yawn, 1=yawn\n","    yawn_classes = [\"no_yawn\", \"yawn\"]\n","    yawn_train, yawn_val = make_gens(DATASET_ROOT, yawn_classes)\n","    print(\"Yawn class_indices:\", yawn_train.class_indices)  # {'no_yawn':0,'yawn':1}\n","    yawn_cw = get_class_weights(yawn_train)\n","    yawn_model = build_base()\n","    yawn_hist = yawn_model.fit(yawn_train, validation_data=yawn_val, epochs=EPOCHS,\n","                               class_weight=yawn_cw, verbose=1)\n","    yawn_h5 = os.path.join(OUTDIR, \"yawn_no_yawn.h5\")\n","    yawn_model.save(yawn_h5); print(\"Saved:\", yawn_h5)\n","    evaluate(yawn_model, yawn_val, \"Yawn (no_yawn vs yawn)\")\n","    to_tflite(yawn_h5)\n","\n","    print(\"\\nAll done. Models in:\", OUTDIR)\n"]},{"cell_type":"code","source":["!pip uninstall -y mediapipe protobuf absl-py\n","!pip cache purge\n","\n","# Versions known to work together on Colab\n","!pip install --no-cache-dir mediapipe==0.10.14 protobuf==3.20.3 absl-py==1.4.0 opencv-python==4.9.0.80\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p545r48YIxep","executionInfo":{"status":"ok","timestamp":1762349629438,"user_tz":-300,"elapsed":12915,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}},"outputId":"839c57ef-d931-4452-b9e9-cc520f1fa4ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping mediapipe as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping protobuf as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: absl-py 1.4.0\n","Uninstalling absl-py-1.4.0:\n","  Successfully uninstalled absl-py-1.4.0\n","Files removed: 39\n","Collecting mediapipe==0.10.14\n","  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Collecting protobuf==3.20.3\n","  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n","Collecting absl-py==1.4.0\n","  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opencv-python==4.9.0.80\n","  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.14) (25.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.14) (25.9.23)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.14) (0.7.2)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.14) (0.7.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.14) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.14) (2.0.2)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.14) (4.12.0.88)\n","INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n","\u001b[31mERROR: Cannot install mediapipe==0.10.14 and protobuf==3.20.3 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n","\u001b[0m\n","The conflict is caused by:\n","    The user requested protobuf==3.20.3\n","    mediapipe 0.10.14 depends on protobuf<5 and >=4.25.3\n","\n","To fix this you could try to:\n","1. loosen the range of package versions you've specified\n","2. remove package versions to allow pip to attempt to solve the dependency conflict\n","\n","\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# Base face landmarker (468 landmarks)\n","!wget -q -O face_landmarker.task \\\n","  https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n"],"metadata":{"id":"cD-r3HCMOedJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade opencv-python mediapipe==0.10.14 tensorflow==2.15.0 pillow\n","# playsound is only useful locally; in Colab we’ll use IPython Audio automatically\n","!pip install playsound==1.3.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KE5tB6bHJAuP","executionInfo":{"status":"ok","timestamp":1762527624211,"user_tz":-300,"elapsed":15250,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}},"outputId":"fb3683a6-15ee-4682-cca7-5a1c17435cff"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: mediapipe==0.10.14 in /usr/local/lib/python3.12/dist-packages (0.10.14)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.15.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.15.0\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: playsound==1.3.0 in /usr/local/lib/python3.12/dist-packages (1.3.0)\n"]}]},{"cell_type":"code","source":["# drowsiness_realtime_fast_fix_video_10s40f_yawncal.py\n","# Colab: capture 10 s in the browser, then uniformly sample 40 frames.\n","# Local: webcam stream. Saves continuous annotated MP4 + IMAGES ONLY on alerts.\n","# Fixes: robust yawn detection via normalized MAR, auto-thresholds, auto-CNN polarity.\n","\n","import os, sys, time, threading, urllib.request, csv, collections, base64\n","import numpy as np\n","import cv2\n","import mediapipe as mp\n","from io import BytesIO\n","from PIL import Image\n","\n","# ===================== USER PATHS ======================\n","EYE_MODEL_H5   = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/eye_open_closed.h5\"\n","YAWN_MODEL_H5  = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/yawn_no_yawn.h5\"\n","\n","USE_TFLITE     = False\n","EYE_TFLITE     = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/eye_open_closed.tflite\"\n","YAWN_TFLITE    = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/yawn_no_yawn.tflite\"\n","\n","ALARM_WAV      = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/mixkit-classic-alarm-995.wav\"\n","\n","ROOT_SAVE      = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/images\"\n","os.makedirs(ROOT_SAVE, exist_ok=True)\n","\n","# Session folder\n","SESSION_TS   = time.strftime(\"%Y%m%d_%H%M%S\")\n","SAVE_DIR     = os.path.join(ROOT_SAVE, f\"session_{SESSION_TS}\")\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# Save policy\n","SAVE_ALERT_IMAGES_ONLY = True\n","VIDEO_FPS_DISK         = 30\n","\n","# ===================== CAPTURE SETTINGS (Colab) =========\n","TARGET_W, TARGET_H, TARGET_FPS = 640, 480, 30\n","CAPTURE_SECONDS  = 10          # browser capture length\n","SAMPLED_FRAMES   = 40          # uniformly sampled frames from those 10 s\n","\n","# ===================== THRESHOLDS (EYES/HEAD) ===========\n","IMG_SIZE = 224\n","EAR_THRESH      = 0.22           # will be refined by auto-calib of p_open\n","HEAD_PITCH_DEG  = 25.0\n","\n","# Instant triggers (per-frame)\n","INSTANT_EAR        = 0.16\n","INSTANT_POPEN_MAX  = 0.40\n","\n","# Voting window for alerts (works with 40 frames)\n","WIN = 5\n","VOTES_TO_ALERT = 3\n","\n","# ===================== ENV / UI HELPERS =================\n","def in_colab():\n","    try:\n","        import google.colab  # type: ignore\n","        return True\n","    except Exception:\n","        return False\n","\n","IN_COLAB = in_colab()\n","\n","def show_frame(frame, win=\"Drowsiness Detector\"):\n","    if IN_COLAB:\n","        from google.colab.patches import cv2_imshow\n","        cv2_imshow(frame)\n","    else:\n","        cv2.imshow(win, frame)\n","\n","def want_stop():\n","    if IN_COLAB:\n","        return False\n","    k = cv2.waitKey(1) & 0xFF\n","    return k in (27, ord('q'))\n","\n","def safe_alarm(path):\n","    try:\n","        if IN_COLAB:\n","            from IPython.display import Audio, display\n","            if os.path.exists(path):\n","                display(Audio(path, autoplay=True))\n","            else:\n","                print(\"\\a\"); time.sleep(0.2); print(\"\\a\")\n","        else:\n","            from playsound import playsound\n","            if os.path.exists(path):\n","                playsound(path)\n","            else:\n","                print(\"\\a\"); time.sleep(0.2); print(\"\\a\")\n","    except Exception:\n","        for _ in range(3):\n","            print(\"\\a\"); time.sleep(0.2)\n","\n","# ===================== MODELS ===========================\n","def load_models():\n","    try:\n","        if USE_TFLITE:\n","            import tensorflow as tf\n","            eye_inter  = tf.lite.Interpreter(model_path=EYE_TFLITE);  eye_inter.allocate_tensors()\n","            yawn_inter = tf.lite.Interpreter(model_path=YAWN_TFLITE); yawn_inter.allocate_tensors()\n","            return dict(mode=\"tflite\",\n","                        eye=eye_inter,  eye_in=eye_inter.get_input_details(),  eye_out=eye_inter.get_output_details(),\n","                        mouth=yawn_inter, mouth_in=yawn_inter.get_input_details(), mouth_out=yawn_inter.get_output_details())\n","        else:\n","            from tensorflow.keras.models import load_model\n","            eye_model  = load_model(EYE_MODEL_H5,  compile=False)\n","            yawn_model = load_model(YAWN_MODEL_H5, compile=False)\n","            return dict(mode=\"keras\", eye=eye_model, mouth=yawn_model)\n","    except Exception as e:\n","        raise RuntimeError(f\"[ModelLoad] Failed to load CNNs: {e}\")\n","\n","def predict_prob(model_pack, roi, eye_model=True):\n","    x = cv2.resize(roi, (IMG_SIZE, IMG_SIZE)).astype(\"float32\")/255.0\n","    x = np.expand_dims(x, 0)\n","    if model_pack[\"mode\"] == \"tflite\":\n","        inter  = model_pack[\"eye\"] if eye_model else model_pack[\"mouth\"]\n","        in_d   = model_pack[\"eye_in\"] if eye_model else model_pack[\"mouth_in\"]\n","        out_d  = model_pack[\"eye_out\"] if eye_model else model_pack[\"mouth_out\"]\n","        inter.set_tensor(in_d[0][\"index\"], x)\n","        inter.invoke()\n","        return float(inter.get_tensor(out_d[0][\"index\"])[0][0])\n","    else:\n","        model = model_pack[\"eye\"] if eye_model else model_pack[\"mouth\"]\n","        return float(model.predict(x, verbose=0)[0][0])\n","\n","# ===================== MEDIAPIPE TASK: LANDMARKER ======\n","from mediapipe.tasks import python as mp_tasks\n","from mediapipe.tasks.python import vision as mp_vision\n","\n","TASK_PATH = \"face_landmarker.task\"\n","TASK_URL  = \"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\"\n","\n","def ensure_task_model():\n","    if not os.path.exists(TASK_PATH):\n","        urllib.request.urlretrieve(TASK_URL, TASK_PATH)\n","\n","def build_landmarker():\n","    ensure_task_model()\n","    BaseOptions = mp_tasks.BaseOptions\n","    FaceLandmarkerOptions = mp_vision.FaceLandmarkerOptions\n","    FaceLandmarker = mp_vision.FaceLandmarker\n","    VisionRunningMode = mp_vision.RunningMode\n","    options = FaceLandmarkerOptions(\n","        base_options=BaseOptions(model_asset_path=TASK_PATH),\n","        running_mode=VisionRunningMode.IMAGE,\n","        num_faces=1\n","    )\n","    return FaceLandmarker.create_from_options(options)\n","\n","def detect_landmarks_468(face_landmarker, frame_bgr):\n","    h, w = frame_bgr.shape[:2]\n","    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB,\n","                        data=cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB))\n","    res = face_landmarker.detect(mp_image)\n","    if not res.face_landmarks:\n","        return None\n","    pts = [(int(p.x*w), int(p.y*h)) for p in res.face_landmarks[0]]\n","    return np.array(pts, dtype=np.int32)\n","\n","# ===================== GEOMETRY / INDICES ===============\n","LEFT_EYE  = [33,160,158,133,153,144]\n","RIGHT_EYE = [263,387,385,362,380,373]\n","MOUTH_4PT = [13,14,78,308]   # top, bottom, left, right\n","LIPS      = [61,291,0,17,13,14,78,308,81,311,402,318,82,312,87,317,178,88,95,185]\n","\n","HP_IDX = {\"nose\":1,\"chin\":199,\"l_eye\":33,\"r_eye\":263,\"l_mouth\":78,\"r_mouth\":308}\n","MODEL_3D = np.array([\n","    (0,0,0),(0,-330,-65),(-225,170,-135),(225,170,-135),(-150,-150,-125),(150,-150,-125)\n","], dtype=np.float64)\n","\n","def euclid(a,b): return np.linalg.norm(np.array(a)-np.array(b))\n","def EAR(eye_pts):\n","    p1,p2,p3,p4,p5,p6 = eye_pts\n","    A = euclid(p2,p6); B = euclid(p3,p5); C = euclid(p1,p4)\n","    return 0.0 if C==0 else (A+B)/(2.0*C)\n","\n","# Robust, scale-normalized mouth opening (MAR_N) using inter-ocular width\n","def MAR_N(mouth4, left_eye_outer, right_eye_outer, eps=1e-6):\n","    top,bottom,left,right = mouth4\n","    v = euclid(top,bottom)\n","    face_w = euclid(left_eye_outer, right_eye_outer)\n","    return v / (face_w + eps)\n","\n","# ===================== VIDEO IO HELPERS ==================\n","def _set_cam_props(cap):\n","    try:\n","        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*\"MJPG\"))\n","        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  TARGET_W)\n","        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, TARGET_H)\n","        cap.set(cv2.CAP_PROP_FPS,          TARGET_FPS)\n","    except Exception:\n","        pass\n","\n","def open_camera_robust(preferred=(0,1,2,3)):\n","    backend_flags = []\n","    if sys.platform.startswith(\"win\"):\n","        backend_flags = [cv2.CAP_DSHOW, cv2.CAP_MSMF]\n","    elif sys.platform == \"darwin\":\n","        backend_flags = [cv2.CAP_AVFOUNDATION]\n","    else:\n","        backend_flags = [cv2.CAP_V4L2]\n","    for idx in preferred:\n","        cap = cv2.VideoCapture(idx)\n","        if cap.isOpened():\n","            _set_cam_props(cap);  return cap\n","        cap.release()\n","        for be in backend_flags:\n","            cap = cv2.VideoCapture(idx, be)\n","            if cap.isOpened():\n","                _set_cam_props(cap);  return cap\n","            cap.release()\n","    return None\n","\n","# ===================== COLAB 10s -> 40f CAPTURE =========\n","def capture_10s_then_sample_40(quality=0.8):\n","    from IPython.display import Javascript, display\n","    from google.colab.output import eval_js\n","\n","    js = Javascript(r\"\"\"\n","      async function robustCapture(seconds, targetW, targetH, targetFps, quality){\n","        const stream = await navigator.mediaDevices.getUserMedia({\n","          video: { width:{ideal:targetW}, height:{ideal:targetH},\n","                   frameRate:{ideal:targetFps, max:targetFps} },\n","          audio: false\n","        });\n","        const video = document.createElement('video');\n","        video.style.display = 'none';\n","        document.body.appendChild(video);\n","        video.srcObject = stream;\n","        await video.play();\n","        let tries = 0;\n","        while ((video.videoWidth === 0 || video.videoHeight === 0) && tries < 60) {\n","          await new Promise(r => setTimeout(r, 100));\n","          tries++;\n","        }\n","        if (video.videoWidth === 0 || video.videoHeight === 0) {\n","          stream.getTracks().forEach(t=>t.stop());\n","          video.remove();\n","          throw new Error(\"Camera not ready (no dimensions).\");\n","        }\n","        const canvas = document.createElement('canvas');\n","        canvas.width = video.videoWidth;\n","        canvas.height = video.videoHeight;\n","        const ctx = canvas.getContext('2d');\n","        const frames = [];\n","        const start = performance.now();\n","        const hardStop = start + (seconds + 2)*1000;\n","        let last = start;\n","        const hasRVFC = ('requestVideoFrameCallback' in HTMLVideoElement.prototype);\n","        function grab(){ ctx.drawImage(video, 0, 0); frames.push(canvas.toDataURL('image/jpeg', quality)); }\n","        async function loop(){\n","          while (true){\n","            const now = performance.now();\n","            if (now - start >= seconds*1000) break;\n","            if (now > hardStop) break;\n","            if (now - last >= 33) { grab(); last = now; }\n","            if (hasRVFC) await new Promise(res => video.requestVideoFrameCallback(()=>res()));\n","            else         await new Promise(res => setTimeout(res, 10));\n","          }\n","        }\n","        try { await loop(); } finally { stream.getTracks().forEach(t=>t.stop()); video.remove(); }\n","        return {frames: frames, w: canvas.width, h: canvas.height};\n","      }\n","    \"\"\")\n","    display(js)\n","\n","    data = eval_js(f\"robustCapture({CAPTURE_SECONDS}, {TARGET_W}, {TARGET_H}, {TARGET_FPS}, {quality})\")\n","    raw = []\n","    for d in data['frames']:\n","        b = d.split(',')[1]\n","        img = Image.open(BytesIO(base64.b64decode(b))).convert('RGB')\n","        raw.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n","    if len(raw) == 0:\n","        raise RuntimeError(\"No frames captured.\")\n","    idxs = np.linspace(0, len(raw)-1, num=SAMPLED_FRAMES, dtype=int)\n","    return [raw[i] for i in idxs]\n","\n","# ===================== LIGHT ENHANCEMENT =================\n","def enhance_full(frame):\n","    if frame.mean() < 60:\n","        yuv = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n","        yuv[:,:,0] = cv2.equalizeHist(yuv[:,:,0])\n","        return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)\n","    return frame\n","\n","def enhance_eye_roi(roi_bgr):\n","    return roi_bgr\n","\n","class EMA:\n","    def __init__(self, alpha=0.7):\n","        self.alpha = alpha; self.v = None\n","    def update(self, x):\n","        self.v = x if self.v is None else self.alpha*self.v + (1-self.alpha)*x\n","        return self.v\n","\n","# ===================== LOGGING ==========================\n","def init_csv_log(csv_path):\n","    if not os.path.exists(csv_path):\n","        with open(csv_path, \"w\", newline=\"\") as f:\n","            w = csv.writer(f)\n","            w.writerow([\"time\", \"frame_idx\", \"EAR\", \"MAR_N\", \"Pitch\", \"p_open_s\", \"p_yawn_s\", \"reason\", \"saved_path\"])\n","\n","# ===================== CORE LOOP ========================\n","def process_frames_iter(frames_iter, models, face_landmarker, win=\"Drowsiness Detector\"):\n","    last_alarm = 0.0\n","    p_open_ema = EMA(0.7); p_yawn_ema = EMA(0.7)\n","\n","    csv_path = os.path.join(SAVE_DIR, \"session_metrics.csv\")\n","    init_csv_log(csv_path)\n","    frame_idx = 0\n","\n","    video_path = os.path.join(SAVE_DIR, f\"session_{SESSION_TS}_annotated.mp4\")\n","    video_writer = None\n","    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","\n","    # rolling votes\n","    eye_buf  = collections.deque(maxlen=WIN)\n","    yawn_buf = collections.deque(maxlen=WIN)\n","    tilt_buf = collections.deque(maxlen=WIN)\n","\n","    # --- Auto-calibration over early frames (for yawn) ---\n","    CAL_FRAMES = 15\n","    yawn_raw_samples = []\n","    marN_samples_closed = []\n","    mouth_polarity = +1  # +1 means model outputs P(yawn); -1 means P(no-yawn) -> we invert\n","    pyawn_thresh = 0.55  # will be updated after calibration\n","    marN_thresh  = 0.07  # default; will be updated after calibration\n","\n","    LAST = {\"ear\":None, \"marN\":None, \"pitch\":0.0, \"popen\":None, \"pyawn\":0.0}\n","\n","    for frame in frames_iter:\n","        if frame is None: break\n","        frame_idx += 1\n","        frame = enhance_full(cv2.flip(frame, 1))\n","        h, w = frame.shape[:2]\n","\n","        if video_writer is None:\n","            video_writer = cv2.VideoWriter(video_path, fourcc, VIDEO_FPS_DISK, (w, h))\n","            print(f\"[Video] Writing annotated MP4 to: {video_path}\")\n","\n","        pts = detect_landmarks_468(face_landmarker, frame)\n","        ear_val = marN_val = None\n","        pitch = 0.0\n","        eye_closed_now = False\n","        yawn_now = False\n","        tilt_now = False\n","        p_open_s = None; p_yawn_s = 0.0\n","\n","        if pts is not None:\n","            # EAR\n","            le = [tuple(pts[i]) for i in LEFT_EYE]\n","            re = [tuple(pts[i]) for i in RIGHT_EYE]\n","            ear_val = (EAR(le) + EAR(re))/2.0\n","            cv2.putText(frame, f\"EAR:{ear_val:.2f}\", (10,26), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0),2)\n","\n","            # Head pitch\n","            try:\n","                ip = np.array([pts[HP_IDX[k]] for k in [\"nose\",\"chin\",\"l_eye\",\"r_eye\",\"l_mouth\",\"r_mouth\"]], dtype=np.float64)\n","                fx = fy = 1.0 * w; cx, cy = w/2.0, h/2.0\n","                cam = np.array([[fx,0,cx],[0,fy,cy],[0,0,1]], dtype=\"double\")\n","                ok, rvec, _ = cv2.solvePnP(MODEL_3D, ip, cam, np.zeros((4,1)), flags=cv2.SOLVEPNP_ITERATIVE)\n","                if ok:\n","                    R, _ = cv2.Rodrigues(rvec); sy = np.sqrt(R[0,0]**2 + R[1,0]**2)\n","                    pitch = np.degrees(np.arctan2(-R[2,0], sy))\n","            except Exception:\n","                pass\n","            cv2.putText(frame, f\"Pitch:{pitch:.1f}\", (10,48), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0),2)\n","\n","            # ROIs and probabilities\n","            def crop_box(points, pad=6):\n","                xs = [p[0] for p in points]; ys = [p[1] for p in points]\n","                x1,x2 = max(0,min(xs)-pad), min(w,max(xs)+pad)\n","                y1,y2 = max(0,min(ys)-pad), min(h,max(ys)+pad)\n","                return frame[y1:y2, x1:x2]\n","\n","            eye_roi   = crop_box(le+re, pad=10)\n","            lips_pts  = [tuple(pts[i]) for i in LIPS]\n","            mouth_roi = crop_box(lips_pts, pad=6)\n","\n","            if eye_roi.size != 0:\n","                p_open = predict_prob(models, enhance_eye_roi(eye_roi), eye_model=True)\n","                p_open_s = p_open_ema.update(p_open)\n","\n","            # --- normalized MAR using outer eye corners (33, 263) as scale ---\n","            mouth4 = [tuple(pts[i]) for i in MOUTH_4PT]\n","            marN_val = MAR_N(mouth4, tuple(pts[33]), tuple(pts[263]))\n","            cv2.putText(frame, f\"MAR_N:{marN_val:.3f}\", (10,70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0),2)\n","\n","            if mouth_roi.size != 0:\n","                p_raw = predict_prob(models, mouth_roi, eye_model=False)\n","                yawn_raw_samples.append(p_raw)\n","                # polarity is decided after CAL_FRAMES using frames with small MAR_N\n","                if mouth_polarity == +1:\n","                    p_yawn = p_raw\n","                else:\n","                    p_yawn = 1.0 - p_raw\n","                p_yawn_s = p_yawn_ema.update(p_yawn)\n","\n","            cv2.putText(frame, f\"Eye(Open)={0.0 if p_open_s is None else p_open_s:.2f}\", (220,26), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0),2)\n","            cv2.putText(frame, f\"Yawn={p_yawn_s:.2f}\", (220,48), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0),2)\n","\n","            # ---------- auto-calibration during first CAL_FRAMES ----------\n","            if frame_idx <= CAL_FRAMES:\n","                # collect MAR_N for presumed-closed mouth frames (small MAR_N)\n","                if marN_val is not None:\n","                    marN_samples_closed.append(marN_val)\n","            elif frame_idx == CAL_FRAMES + 1:\n","                # decide polarity: when mouth likely closed, P(yawn) should be low\n","                if len(yawn_raw_samples) > 5 and len(marN_samples_closed) > 3:\n","                    mean_raw = float(np.mean(yawn_raw_samples))\n","                    # if mean_raw is high with closed mouth, invert\n","                    mouth_polarity = -1 if mean_raw > 0.5 else +1\n","                # set thresholds: closed median + delta; and a moderate pyawn threshold\n","                med_marN = float(np.median(marN_samples_closed)) if marN_samples_closed else 0.02\n","                marN_thresh = np.clip(med_marN + 0.02, 0.04, 0.12)\n","                pyawn_thresh = 0.55  # conservative default\n","            # --------------------------------------------------------------\n","\n","            # Eyes fusion + instant\n","            if (ear_val is not None) and (p_open_s is not None):\n","                eye_closed_now = ((ear_val < EAR_THRESH and p_open_s < 0.45) or\n","                                  (ear_val < INSTANT_EAR and p_open_s < INSTANT_POPEN_MAX))\n","            elif ear_val is not None:\n","                eye_closed_now = ear_val < INSTANT_EAR\n","            elif p_open_s is not None:\n","                eye_closed_now = p_open_s < INSTANT_POPEN_MAX\n","\n","            # Yawn fusion using calibrated thresholds\n","            # If calibration hasn't run yet, fall back to safe defaults.\n","            mar_thr = locals().get(\"marN_thresh\", 0.08)\n","            py_thr  = locals().get(\"pyawn_thresh\", 0.55)\n","            yawn_now = ((marN_val is not None and marN_val > mar_thr) or\n","                        (p_yawn_s is not None and p_yawn_s > py_thr))\n","\n","            tilt_now = abs(pitch) > HEAD_PITCH_DEG\n","\n","            LAST.update({\"ear\":ear_val, \"marN\":marN_val, \"pitch\":pitch, \"popen\":p_open_s, \"pyawn\":p_yawn_s})\n","        else:\n","            # brief fallback using last good state\n","            ear_val = LAST[\"ear\"]; marN_val = LAST[\"marN\"]; pitch = LAST[\"pitch\"]\n","            p_open_s = LAST[\"popen\"]; p_yawn_s = LAST[\"pyawn\"]\n","            mar_thr = locals().get(\"marN_thresh\", 0.08)\n","            py_thr  = locals().get(\"pyawn_thresh\", 0.55)\n","            eye_closed_now = (ear_val is not None and p_open_s is not None and ear_val < EAR_THRESH and p_open_s < 0.45)\n","            yawn_now = ((p_yawn_s is not None and p_yawn_s > py_thr) or\n","                        (marN_val is not None and marN_val > mar_thr))\n","            tilt_now = abs(pitch) > HEAD_PITCH_DEG\n","            cv2.putText(frame, \"Face lost… using last state\", (10,92), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n","\n","        # votes\n","        eye_buf.append(bool(eye_closed_now))\n","        yawn_buf.append(bool(yawn_now))\n","        tilt_buf.append(bool(tilt_now))\n","\n","        reason = None\n","        if sum(eye_buf)  >= VOTES_TO_ALERT: reason = \"Eyes closed\"\n","        elif sum(yawn_buf) >= VOTES_TO_ALERT: reason = \"Yawning\"\n","        elif sum(tilt_buf) >= VOTES_TO_ALERT: reason = \"Head tilt\"\n","\n","        if reason:\n","            cv2.putText(frame, f\"DROWSINESS ALERT: {reason}\", (40,140),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n","            now = time.time()\n","            if now - last_alarm > 5.0:\n","                last_alarm = now\n","                threading.Thread(target=safe_alarm, args=(ALARM_WAV,), daemon=True).start()\n","\n","        # write video every frame\n","        if video_writer is not None:\n","            video_writer.write(frame)\n","\n","        # save alert frame\n","        img_path = \"\"\n","        if SAVE_ALERT_IMAGES_ONLY and reason:\n","            ts = time.strftime(\"%Y%m%d_%H%M%S\")\n","            img_path = os.path.join(SAVE_DIR, f\"alert_{reason.replace(' ','_')}_{ts}_{frame_idx:06d}.jpg\")\n","            cv2.imwrite(img_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 85])\n","\n","        # CSV log\n","        ts_now = time.strftime(\"%Y%m%d_%H%M%S\")\n","        with open(csv_path, \"a\", newline=\"\") as f:\n","            w = csv.writer(f)\n","            w.writerow([ts_now, frame_idx,\n","                        None if ear_val is None else f\"{ear_val:.3f}\",\n","                        None if marN_val is None else f\"{marN_val:.3f}\",\n","                        None if pitch   is None else f\"{pitch:.1f}\",\n","                        None if p_open_ema.v is None else f\"{p_open_ema.v:.3f}\",\n","                        None if p_yawn_ema.v is None else f\"{p_yawn_ema.v:.3f}\",\n","                        \"\" if reason is None else reason,\n","                        img_path])\n","\n","        show_frame(frame)\n","        if want_stop():\n","            break\n","\n","    if video_writer is not None:\n","        video_writer.release()\n","        print(f\"[Video] Saved: {video_path}\")\n","    if not IN_COLAB:\n","        cv2.destroyAllWindows()\n","\n","# ===================== MAIN =============================\n","def main():\n","    global TARGET_W, TARGET_H, TARGET_FPS, CAPTURE_SECONDS, SAMPLED_FRAMES\n","    models = load_models()\n","    landmarker = build_landmarker()\n","\n","    cap = None if IN_COLAB else open_camera_robust()\n","    if (cap is not None) and cap.isOpened():\n","        def webcam_frames():\n","            while True:\n","                ok, f = cap.read()\n","                if not ok: break\n","                yield f\n","        try:\n","            process_frames_iter(webcam_frames(), models, landmarker)\n","        finally:\n","            cap.release()\n","        return\n","\n","    if IN_COLAB:\n","        print(\"[Info] Colab: capturing 10 s then sampling 40 frames …\")\n","        frames = capture_10s_then_sample_40(quality=0.8)\n","        process_frames_iter(frames, models, landmarker)\n","        return\n","\n","    raise RuntimeError(\"Cannot open webcam. If local, check camera permissions or indices/backends.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1JB6H3EPxzKbU7BI1wbeB3qENchkkoZ2_"},"id":"xLp_JzeIFYi9","outputId":"9d78f446-a970-4bf1-fe26-396ebbb421a6","executionInfo":{"status":"ok","timestamp":1762527868669,"user_tz":-300,"elapsed":40525,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}}},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# Colab webcam capture helper (grabs N frames through JS)\n","from IPython.display import Javascript, display\n","from google.colab.output import eval_js\n","import numpy as np, cv2, base64\n","from PIL import Image\n","from io import BytesIO\n","\n","def capture_frames_colab(n_frames=120, quality=0.8):\n","    js = Javascript('''\n","    async function captureFrames(n, quality) {\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","      const video = document.createElement('video');\n","      document.body.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      const ctx = canvas.getContext('2d');\n","\n","      let frames = [];\n","      for (let i=0; i<n; i++) {\n","        ctx.drawImage(video, 0, 0);\n","        const dataURL = canvas.toDataURL('image/jpeg', quality);\n","        frames.push(dataURL);\n","        await new Promise(r => setTimeout(r, 33)); // ~30 fps\n","      }\n","      stream.getTracks().forEach(t => t.stop());\n","      video.remove();\n","      return frames;\n","    }''')\n","    display(js)\n","    data = eval_js(f'captureFrames({n_frames}, {quality})')\n","    frames = []\n","    for d in data:\n","        b = d.split(',')[1]\n","        img = Image.open(BytesIO(base64.b64decode(b))).convert('RGB')\n","        frames.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n","    return frames\n","\n","# Example usage:\n","frames = capture_frames_colab(180, 0.8)   # capture ~6 seconds\n","len(frames), frames[0].shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"lhM7eqo_MssL","executionInfo":{"status":"ok","timestamp":1762350896244,"user_tz":-300,"elapsed":14313,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}},"outputId":"3acfe9d7-8b1c-4484-cf54-f88207e08579"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function captureFrames(n, quality) {\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","      const video = document.createElement('video');\n","      document.body.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      const ctx = canvas.getContext('2d');\n","\n","      let frames = [];\n","      for (let i=0; i<n; i++) {\n","        ctx.drawImage(video, 0, 0);\n","        const dataURL = canvas.toDataURL('image/jpeg', quality);\n","        frames.push(dataURL);\n","        await new Promise(r => setTimeout(r, 33)); // ~30 fps\n","      }\n","      stream.getTracks().forEach(t => t.stop());\n","      video.remove();\n","      return frames;\n","    }"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["(180, (480, 640, 3))"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# drowsiness_realtime_fast_fix_video_10s40f.py\n","# Colab: capture 10 s in the browser, then uniformly sample 40 frames.\n","# Local: webcam stream. Saves continuous annotated MP4 + images ONLY on alerts.\n","\n","import os, sys, time, threading, urllib.request, csv, collections, base64\n","import numpy as np\n","import cv2\n","import mediapipe as mp\n","from io import BytesIO\n","from PIL import Image\n","\n","# ===================== USER PATHS ======================\n","EYE_MODEL_H5   = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/eye_open_closed.h5\"\n","YAWN_MODEL_H5  = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/yawn_no_yawn.h5\"\n","\n","USE_TFLITE     = False\n","EYE_TFLITE     = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/eye_open_closed.tflite\"\n","YAWN_TFLITE    = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/yawn_no_yawn.tflite\"\n","\n","ALARM_WAV      = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/mixkit-classic-alarm-995.wav\"\n","\n","ROOT_SAVE      = \"/content/drive/MyDrive/Colab Notebooks/tools & technique/resume & porfolio projects/drowniss_project/images\"\n","os.makedirs(ROOT_SAVE, exist_ok=True)\n","\n","# Session folder\n","SESSION_TS   = time.strftime(\"%Y%m%d_%H%M%S\")\n","SAVE_DIR     = os.path.join(ROOT_SAVE, f\"session_{SESSION_TS}\")\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# Save policy\n","SAVE_ALERT_IMAGES_ONLY = True\n","VIDEO_FPS_DISK         = 30\n","\n","# ===================== CAPTURE SETTINGS (Colab) =========\n","TARGET_W, TARGET_H, TARGET_FPS = 640, 480, 30\n","CAPTURE_SECONDS  = 10          # browser capture length\n","SAMPLED_FRAMES   = 40          # uniformly sampled frames from those 10 s\n","\n","# ===================== THRESHOLDS ======================\n","IMG_SIZE = 224\n","EAR_THRESH      = 0.22\n","MAR_THRESH      = 0.60\n","HEAD_PITCH_DEG  = 25.0\n","\n","# Instant triggers\n","INSTANT_EAR        = 0.16\n","INSTANT_POPEN_MAX  = 0.40\n","INSTANT_MAR        = 0.80\n","INSTANT_PYAWN_MIN  = 0.75\n","\n","# Short rolling vote window (works with 40 frames)\n","WIN = 5\n","VOTES_TO_ALERT = 3\n","\n","MOUTH_MODEL_RETURNS_P_YAWN = False\n","\n","# ===================== ENV / UI HELPERS =================\n","def in_colab():\n","    try:\n","        import google.colab  # type: ignore\n","        return True\n","    except Exception:\n","        return False\n","\n","IN_COLAB = in_colab()\n","\n","def show_frame(frame, win=\"Drowsiness Detector\"):\n","    if IN_COLAB:\n","        from google.colab.patches import cv2_imshow\n","        cv2_imshow(frame)\n","    else:\n","        cv2.imshow(win, frame)\n","\n","def want_stop():\n","    if IN_COLAB:\n","        return False\n","    k = cv2.waitKey(1) & 0xFF\n","    return k in (27, ord('q'))\n","\n","def safe_alarm(path):\n","    try:\n","        if IN_COLAB:\n","            from IPython.display import Audio, display\n","            if os.path.exists(path):\n","                display(Audio(path, autoplay=True))\n","            else:\n","                print(\"\\a\"); time.sleep(0.2); print(\"\\a\")\n","        else:\n","            from playsound import playsound\n","            if os.path.exists(path):\n","                playsound(path)\n","            else:\n","                print(\"\\a\"); time.sleep(0.2); print(\"\\a\")\n","    except Exception:\n","        for _ in range(3):\n","            print(\"\\a\"); time.sleep(0.2)\n","\n","# ===================== MODELS ===========================\n","def load_models():\n","    try:\n","        if USE_TFLITE:\n","            import tensorflow as tf\n","            eye_inter  = tf.lite.Interpreter(model_path=EYE_TFLITE);  eye_inter.allocate_tensors()\n","            yawn_inter = tf.lite.Interpreter(model_path=YAWN_TFLITE); yawn_inter.allocate_tensors()\n","            return dict(mode=\"tflite\",\n","                        eye=eye_inter,  eye_in=eye_inter.get_input_details(),  eye_out=eye_inter.get_output_details(),\n","                        mouth=yawn_inter, mouth_in=yawn_inter.get_input_details(), mouth_out=yawn_inter.get_output_details())\n","        else:\n","            from tensorflow.keras.models import load_model\n","            eye_model  = load_model(EYE_MODEL_H5,  compile=False)\n","            yawn_model = load_model(YAWN_MODEL_H5, compile=False)\n","            return dict(mode=\"keras\", eye=eye_model, mouth=yawn_model)\n","    except Exception as e:\n","        raise RuntimeError(f\"[ModelLoad] Failed to load CNNs: {e}\")\n","\n","def predict_prob(model_pack, roi, eye_model=True):\n","    x = cv2.resize(roi, (IMG_SIZE, IMG_SIZE)).astype(\"float32\")/255.0\n","    x = np.expand_dims(x, 0)\n","    if model_pack[\"mode\"] == \"tflite\":\n","        inter  = model_pack[\"eye\"] if eye_model else model_pack[\"mouth\"]\n","        in_d   = model_pack[\"eye_in\"] if eye_model else model_pack[\"mouth_in\"]\n","        out_d  = model_pack[\"eye_out\"] if eye_model else model_pack[\"mouth_out\"]\n","        inter.set_tensor(in_d[0][\"index\"], x)\n","        inter.invoke()\n","        return float(inter.get_tensor(out_d[0][\"index\"])[0][0])\n","    else:\n","        model = model_pack[\"eye\"] if eye_model else model_pack[\"mouth\"]\n","        return float(model.predict(x, verbose=0)[0][0])\n","\n","# ===================== MEDIAPIPE TASK: LANDMARKER ======\n","from mediapipe.tasks import python as mp_tasks\n","from mediapipe.tasks.python import vision as mp_vision\n","\n","TASK_PATH = \"face_landmarker.task\"\n","TASK_URL  = \"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\"\n","\n","def ensure_task_model():\n","    if not os.path.exists(TASK_PATH):\n","        urllib.request.urlretrieve(TASK_URL, TASK_PATH)\n","\n","def build_landmarker():\n","    ensure_task_model()\n","    BaseOptions = mp_tasks.BaseOptions\n","    FaceLandmarkerOptions = mp_vision.FaceLandmarkerOptions\n","    FaceLandmarker = mp_vision.FaceLandmarker\n","    VisionRunningMode = mp_vision.RunningMode\n","    options = FaceLandmarkerOptions(\n","        base_options=BaseOptions(model_asset_path=TASK_PATH),\n","        running_mode=VisionRunningMode.IMAGE,\n","        num_faces=1\n","    )\n","    return FaceLandmarker.create_from_options(options)\n","\n","def detect_landmarks_468(face_landmarker, frame_bgr):\n","    h, w = frame_bgr.shape[:2]\n","    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB,\n","                        data=cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB))\n","    res = face_landmarker.detect(mp_image)\n","    if not res.face_landmarks:\n","        return None\n","    pts = [(int(p.x*w), int(p.y*h)) for p in res.face_landmarks[0]]\n","    return np.array(pts, dtype=np.int32)\n","\n","# ===================== GEOMETRY / INDICES ===============\n","LEFT_EYE  = [33,160,158,133,153,144]\n","RIGHT_EYE = [263,387,385,362,380,373]\n","MOUTH_4PT = [13,14,78,308]\n","LIPS      = [61,291,0,17,13,14,78,308,81,311,402,318,82,312,87,317,178,88,95,185]\n","\n","HP_IDX = {\"nose\":1,\"chin\":199,\"l_eye\":33,\"r_eye\":263,\"l_mouth\":78,\"r_mouth\":308}\n","MODEL_3D = np.array([\n","    (0,0,0),(0,-330,-65),(-225,170,-135),(225,170,-135),(-150,-150,-125),(150,-150,-125)\n","], dtype=np.float64)\n","\n","def euclid(a,b): return np.linalg.norm(np.array(a)-np.array(b))\n","def EAR(eye_pts):\n","    p1,p2,p3,p4,p5,p6 = eye_pts\n","    A = euclid(p2,p6); B = euclid(p3,p5); C = euclid(p1,p4)\n","    return 0.0 if C==0 else (A+B)/(2.0*C)\n","def MAR(m4):\n","    top,bottom,left,right = m4\n","    v,h = euclid(top,bottom), euclid(left,right)\n","    return 0.0 if h==0 else v/h\n","\n","# ===================== VIDEO IO HELPERS ==================\n","def _set_cam_props(cap):\n","    try:\n","        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*\"MJPG\"))\n","        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  TARGET_W)\n","        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, TARGET_H)\n","        cap.set(cv2.CAP_PROP_FPS,          TARGET_FPS)\n","    except Exception:\n","        pass\n","\n","def open_camera_robust(preferred=(0,1,2,3)):\n","    backend_flags = []\n","    if sys.platform.startswith(\"win\"):\n","        backend_flags = [cv2.CAP_DSHOW, cv2.CAP_MSMF]\n","    elif sys.platform == \"darwin\":\n","        backend_flags = [cv2.CAP_AVFOUNDATION]\n","    else:\n","        backend_flags = [cv2.CAP_V4L2]\n","    for idx in preferred:\n","        cap = cv2.VideoCapture(idx)\n","        if cap.isOpened():\n","            _set_cam_props(cap);  return cap\n","        cap.release()\n","        for be in backend_flags:\n","            cap = cv2.VideoCapture(idx, be)\n","            if cap.isOpened():\n","                _set_cam_props(cap);  return cap\n","            cap.release()\n","    return None\n","\n","# ===================== COLAB 10s -> 40f CAPTURE =========\n","def capture_10s_then_sample_40(quality=0.8):\n","    \"\"\"Robust 10 s browser capture; uniformly sample exactly 40 frames.\"\"\"\n","    from IPython.display import Javascript, display\n","    from google.colab.output import eval_js\n","\n","    js = Javascript(r\"\"\"\n","      async function robustCapture(seconds, targetW, targetH, targetFps, quality){\n","        const stream = await navigator.mediaDevices.getUserMedia({\n","          video: { width:{ideal:targetW}, height:{ideal:targetH},\n","                   frameRate:{ideal:targetFps, max:targetFps} },\n","          audio: false\n","        });\n","\n","        const video = document.createElement('video');\n","        video.style.display = 'none';\n","        document.body.appendChild(video);\n","        video.srcObject = stream;\n","        await video.play();\n","\n","        // Wait for real dimensions\n","        let tries = 0;\n","        while ((video.videoWidth === 0 || video.videoHeight === 0) && tries < 60) {\n","          await new Promise(r => setTimeout(r, 100));\n","          tries++;\n","        }\n","        if (video.videoWidth === 0 || video.videoHeight === 0) {\n","          stream.getTracks().forEach(t=>t.stop());\n","          video.remove();\n","          throw new Error(\"Camera not ready (no dimensions).\");\n","        }\n","\n","        const canvas = document.createElement('canvas');\n","        canvas.width = video.videoWidth;\n","        canvas.height = video.videoHeight;\n","        const ctx = canvas.getContext('2d');\n","\n","        const frames = [];\n","        const start = performance.now();\n","        const hardStop = start + (seconds + 2)*1000;\n","        let last = start;\n","        const hasRVFC = ('requestVideoFrameCallback' in HTMLVideoElement.prototype);\n","\n","        function grab(){\n","          ctx.drawImage(video, 0, 0);\n","          frames.push(canvas.toDataURL('image/jpeg', quality));\n","        }\n","\n","        async function loop(){\n","          while (true){\n","            const now = performance.now();\n","            if (now - start >= seconds*1000) break;\n","            if (now > hardStop) break;\n","\n","            const elapsed = now - last;\n","            if (elapsed >= 1000/30 - 1) { grab(); last = now; }\n","\n","            if (hasRVFC) {\n","              await new Promise(res => video.requestVideoFrameCallback(()=>res()));\n","            } else {\n","              await new Promise(res => setTimeout(res, 10));\n","            }\n","          }\n","        }\n","\n","        try { await loop(); } finally {\n","          stream.getTracks().forEach(t=>t.stop());\n","          video.remove();\n","        }\n","        return {frames: frames, w: canvas.width, h: canvas.height};\n","      }\n","    \"\"\")\n","    display(js)\n","\n","    data = eval_js(f\"robustCapture({CAPTURE_SECONDS}, {TARGET_W}, {TARGET_H}, {TARGET_FPS}, {quality})\")\n","    raw = []\n","    for d in data['frames']:\n","        b = d.split(',')[1]\n","        img = Image.open(BytesIO(base64.b64decode(b))).convert('RGB')\n","        raw.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n","\n","    if len(raw) == 0:\n","        raise RuntimeError(\"No frames captured. Keep the Colab tab focused; allow camera; close other apps using camera.\")\n","\n","    # Uniform downsample to SAMPLED_FRAMES\n","    idxs = np.linspace(0, len(raw)-1, num=SAMPLED_FRAMES, dtype=int)\n","    return [raw[i] for i in idxs]\n","\n","# ===================== LIGHT ENHANCEMENT =================\n","def enhance_full(frame):\n","    if frame.mean() < 60:\n","        yuv = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n","        yuv[:,:,0] = cv2.equalizeHist(yuv[:,:,0])\n","        return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)\n","    return frame\n","\n","def enhance_eye_roi(roi_bgr):\n","    return roi_bgr\n","\n","class EMA:\n","    def __init__(self, alpha=0.7):\n","        self.alpha = alpha; self.v = None\n","    def update(self, x):\n","        self.v = x if self.v is None else self.alpha*self.v + (1-self.alpha)*x\n","        return self.v\n","\n","# ===================== LOGGING ==========================\n","def init_csv_log(csv_path):\n","    if not os.path.exists(csv_path):\n","        with open(csv_path, \"w\", newline=\"\") as f:\n","            w = csv.writer(f)\n","            w.writerow([\"time\", \"frame_idx\", \"EAR\", \"MAR\", \"Pitch\", \"p_open_s\", \"p_yawn_s\", \"reason\", \"saved_path\"])\n","\n","# ===================== CORE LOOP ========================\n","def process_frames_iter(frames_iter, models, face_landmarker, win=\"Drowsiness Detector\"):\n","    last_alarm = 0.0\n","    p_open_ema = EMA(0.7); p_yawn_ema = EMA(0.7)\n","\n","    csv_path = os.path.join(SAVE_DIR, \"session_metrics.csv\")\n","    init_csv_log(csv_path)\n","    frame_idx = 0\n","\n","    # Annotated video writer\n","    video_path = os.path.join(SAVE_DIR, f\"session_{SESSION_TS}_annotated.mp4\")\n","    video_writer = None\n","    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","\n","    # rolling votes\n","    eye_buf  = collections.deque(maxlen=WIN)\n","    yawn_buf = collections.deque(maxlen=WIN)\n","    tilt_buf = collections.deque(maxlen=WIN)\n","\n","    LAST = {\"ear\":None, \"mar\":None, \"pitch\":0.0, \"popen\":None, \"pyawn\":0.0}\n","\n","    for frame in frames_iter:\n","        if frame is None: break\n","        frame_idx += 1\n","        frame = enhance_full(cv2.flip(frame, 1))\n","        h, w = frame.shape[:2]\n","\n","        if video_writer is None:\n","            video_writer = cv2.VideoWriter(video_path, fourcc, VIDEO_FPS_DISK, (w, h))\n","            print(f\"[Video] Writing annotated MP4 to: {video_path}\")\n","\n","        pts = detect_landmarks_468(face_landmarker, frame)\n","        ear_val = mar_val = None\n","        pitch = 0.0\n","        eye_closed_now = False\n","        yawn_now = False\n","        tilt_now = False\n","\n","        if pts is not None:\n","            le = [tuple(pts[i]) for i in LEFT_EYE]\n","            re = [tuple(pts[i]) for i in RIGHT_EYE]\n","            ear_val = (EAR(le) + EAR(re))/2.0\n","            cv2.putText(frame, f\"EAR:{ear_val:.2f}\", (10,26), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0),2)\n","\n","            mouth4 = [tuple(pts[i]) for i in MOUTH_4PT]\n","            mar_val = MAR(mouth4)\n","            cv2.putText(frame, f\"MAR:{mar_val:.2f}\", (10,48), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0),2)\n","\n","            try:\n","                ip = np.array([pts[HP_IDX[k]] for k in [\"nose\",\"chin\",\"l_eye\",\"r_eye\",\"l_mouth\",\"r_mouth\"]], dtype=np.float64)\n","                fx = fy = 1.0 * w; cx, cy = w/2.0, h/2.0\n","                cam = np.array([[fx,0,cx],[0,fy,cy],[0,0,1]], dtype=\"double\")\n","                ok, rvec, _ = cv2.solvePnP(MODEL_3D, ip, cam, np.zeros((4,1)), flags=cv2.SOLVEPNP_ITERATIVE)\n","                if ok:\n","                    R, _ = cv2.Rodrigues(rvec)\n","                    sy = np.sqrt(R[0,0]**2 + R[1,0]**2)\n","                    pitch = np.degrees(np.arctan2(-R[2,0], sy))\n","                cv2.putText(frame, f\"Pitch:{pitch:.1f}\", (10,70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0),2)\n","            except Exception:\n","                pass\n","\n","            def crop_box(points, pad=6):\n","                xs = [p[0] for p in points]; ys = [p[1] for p in points]\n","                x1,x2 = max(0,min(xs)-pad), min(w,max(xs)+pad)\n","                y1,y2 = max(0,min(ys)-pad), min(h,max(ys)+pad)\n","                return frame[y1:y2, x1:x2]\n","\n","            eye_roi   = crop_box(le+re, pad=10)\n","            lips_pts  = [tuple(pts[i]) for i in LIPS]\n","            mouth_roi = crop_box(lips_pts, pad=6)\n","\n","            p_open_s = None\n","            if eye_roi.size != 0:\n","                p_open = predict_prob(models, enhance_eye_roi(eye_roi), eye_model=True)\n","                p_open_s = p_open_ema.update(p_open)\n","                cv2.putText(frame, f\"Eye(Open)={p_open_s:.2f}\", (220,26), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0),2)\n","\n","            p_yawn_s = 0.0\n","            if mouth_roi.size != 0:\n","                p_raw = predict_prob(models, mouth_roi, eye_model=False)\n","                p_yawn = (p_raw if MOUTH_MODEL_RETURNS_P_YAWN else (1.0 - p_raw))\n","                if mar_val is not None and mar_val < 0.20:\n","                    p_yawn = 0.0\n","                p_yawn_s = p_yawn_ema.update(p_yawn)\n","                cv2.putText(frame, f\"Yawn={p_yawn_s:.2f}\", (220,48), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0),2)\n","\n","            # Instant rules for this frame\n","            if ear_val is not None and p_open_s is not None:\n","                eye_closed_now = ((ear_val < EAR_THRESH and p_open_s < 0.45) or\n","                                  (ear_val < INSTANT_EAR and p_open_s < INSTANT_POPEN_MAX))\n","            elif ear_val is not None:\n","                eye_closed_now = ear_val < INSTANT_EAR\n","            elif p_open_s is not None:\n","                eye_closed_now = p_open_s < INSTANT_POPEN_MAX\n","\n","            yawn_now = ((mar_val is not None and mar_val > MAR_THRESH) or (p_yawn_s > 0.5) or\n","                        (mar_val is not None and mar_val > INSTANT_MAR) or (p_yawn_s > INSTANT_PYAWN_MIN))\n","\n","            tilt_now = abs(pitch) > HEAD_PITCH_DEG\n","\n","            LAST.update({\"ear\":ear_val, \"mar\":mar_val, \"pitch\":pitch, \"popen\":p_open_s, \"pyawn\":p_yawn_s})\n","        else:\n","            # brief fallback\n","            ear_val = LAST[\"ear\"]; mar_val = LAST[\"mar\"]; pitch = LAST[\"pitch\"]\n","            p_open_s = LAST[\"popen\"]; p_yawn_s = LAST[\"pyawn\"]\n","            eye_closed_now = (ear_val is not None and p_open_s is not None and ear_val < EAR_THRESH and p_open_s < 0.45)\n","            yawn_now = (p_yawn_s is not None and p_yawn_s > 0.5) or (mar_val is not None and mar_val > MAR_THRESH)\n","            tilt_now = abs(pitch) > HEAD_PITCH_DEG\n","            cv2.putText(frame, \"Face lost… using last state\", (10,92), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n","\n","        # votes\n","        eye_buf.append(eye_closed_now)\n","        yawn_buf.append(yawn_now)\n","        tilt_buf.append(tilt_now)\n","\n","        reason = None\n","        if sum(eye_buf)  >= VOTES_TO_ALERT: reason = \"Eyes closed\"\n","        elif sum(yawn_buf) >= VOTES_TO_ALERT: reason = \"Yawning\"\n","        elif sum(tilt_buf) >= VOTES_TO_ALERT: reason = \"Head tilt\"\n","\n","        if reason:\n","            cv2.putText(frame, f\"DROWSINESS ALERT: {reason}\", (40,140),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n","            now = time.time()\n","            if now - last_alarm > 5.0:\n","                last_alarm = now\n","                threading.Thread(target=safe_alarm, args=(ALARM_WAV,), daemon=True).start()\n","\n","        # write video every frame\n","        if video_writer is not None:\n","            video_writer.write(frame)\n","\n","        # save alert frame\n","        img_path = \"\"\n","        if SAVE_ALERT_IMAGES_ONLY and reason:\n","            ts = time.strftime(\"%Y%m%d_%H%M%S\")\n","            img_path = os.path.join(SAVE_DIR, f\"alert_{reason.replace(' ','_')}_{ts}_{frame_idx:06d}.jpg\")\n","            cv2.imwrite(img_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 85])\n","\n","        # CSV log\n","        ts_now = time.strftime(\"%Y%m%d_%H%M%S\")\n","        with open(csv_path, \"a\", newline=\"\") as f:\n","            w = csv.writer(f)\n","            w.writerow([ts_now, frame_idx,\n","                        None if ear_val is None else f\"{ear_val:.3f}\",\n","                        None if mar_val is None else f\"{mar_val:.3f}\",\n","                        None if pitch   is None else f\"{pitch:.1f}\",\n","                        None if p_open_ema.v is None else f\"{p_open_ema.v:.3f}\",\n","                        None if p_yawn_ema.v is None else f\"{p_yawn_ema.v:.3f}\",\n","                        \"\" if reason is None else reason,\n","                        img_path])\n","\n","        show_frame(frame)\n","        if want_stop():\n","            break\n","\n","    if video_writer is not None:\n","        video_writer.release()\n","        print(f\"[Video] Saved: {video_path}\")\n","    if not IN_COLAB:\n","        cv2.destroyAllWindows()\n","\n","# ===================== MAIN =============================\n","def main():\n","    models = load_models()\n","    landmarker = build_landmarker()\n","\n","    cap = None if IN_COLAB else open_camera_robust()\n","    if (cap is not None) and cap.isOpened():\n","        def webcam_frames():\n","            while True:\n","                ok, f = cap.read()\n","                if not ok: break\n","                yield f\n","        try:\n","            process_frames_iter(webcam_frames(), models, landmarker)\n","        finally:\n","            cap.release()\n","        return\n","\n","    if IN_COLAB:\n","        print(\"[Info] Colab: capturing 10 s then sampling 40 frames …\")\n","        frames = capture_10s_then_sample_40(quality=0.8)\n","        process_frames_iter(frames, models, landmarker)\n","        return\n","\n","    raise RuntimeError(\"Cannot open webcam. If local, check camera permissions or indices/backends.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1QQLYPwDmp93wp7ABzEQ9bjs5p96DlJRg"},"id":"u1wAwRFPHFco","executionInfo":{"status":"ok","timestamp":1762417930773,"user_tz":-300,"elapsed":44447,"user":{"displayName":"Zabhi Uddin","userId":"08852294498978454973"}},"outputId":"d3cde5e8-629e-41fd-c64a-9f26016d5cb0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}
